{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks - Graph view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* understand why we need non-linearities\n",
    "* get to know the most common activation functions\n",
    "* understand the computational graph abstraction and how it relates to the training procedure of neural networks\n",
    "* know the biggest difference in traditional and deep learning feature representations  [dense vs one-hot representations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have seen how a neural network can be formalized, both algebraically and graphically. We can think of a feed-forward NN as a function $NN(\\mathbf{x})$: $$y= NN(\\mathbf{x}) $$\n",
    "\n",
    "with:\n",
    "* input: $\\mathbf{x}$ (vector with $d_{in}$ dimensions)\n",
    "* output: $\\mathbf{y}$ (output with $d_{out}$ dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall our network: <img src=\"pics/nn.png\"> \n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that you can also see it as a set of vector-matrix operations:\n",
    "\n",
    "* In the first layer, the input of 4 dimensions ($x_{1x4}$) is transformed into a vector of 5 dimensions:\n",
    "\n",
    "$$\\textbf{x}_{1x4} \\cdot \\textbf{W}^1_{4x5} \\rightarrow \\textbf{v}_{1x5}$$\n",
    "\n",
    "to which the bias vector is added, and the whole is sent through the activation function to calculate the hidden layer values:\n",
    "\n",
    "$$\\textbf{v}_{1x5} + \\textbf{b}_{1x5} \\rightarrow \\mbox{apply } g \\rightarrow \\textbf{h}_{1x5}$$\n",
    "\n",
    "and so forth until the end:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$\\mathbf{h1}=g(\\mathbf{xW^1+b^1})$$\n",
    "$$NN_{MLP1}(\\mathbf{x})=\\mathbf{h1}\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "Which sizes do $\\mathbf{W^2}$ and $\\mathbf{b^2}$ have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "answer: W2 = 5x1 b2 = 1x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What we just did (calculate the output from the input) is also called the **forward pass** in a neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.43502835  1.16713335 -2.00689194 -1.61879718]\n",
      "[[0.5]]\n",
      "the network has no weights yet..\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# forward-pass of a simple neural network:\n",
    "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (here we use sigmoid)\n",
    "x = np.random.randn(4) # random input vector of four numbers (1x4) \n",
    "print(x)\n",
    "W1 = np.zeros((4,5))   # Weights W1 (4x5). Note that the weights are commonly initialised randomly, e.g. with Numpy's randn\n",
    "W2 = np.zeros((5,1))   # Weights W2 (5x1)\n",
    "b1 = np.zeros((1,5))\n",
    "b2 = np.zeros((1,1))\n",
    "h1=f(np.dot(x,W1)+b1) # calculate the activations of the first hidden layer (1x5) - linear transformation followed by non-linearity!\n",
    "out=f(np.dot(h1,W2)+b2) # calculate output (1x1)\n",
    "print(out)\n",
    "print(\"the network has no weights yet..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All $\\mathbf{W}$ and $\\mathbf{b}$ are the **parameters** (or **weights**) of the model. They are commonly referred to as θ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Intuition\n",
    "We want to adjust the weights so that *a small change in the input should have a small effect in the output*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In case of a simple model, such as a simple perceptron, a small change might often have a large effect. Remember, the decision function for the perceptron is a threshold, this can be seen as a **step function**: <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ac/HardLimitFunction.png\" width=300> \n",
    "\n",
    "It is 0 for everything below 0 and 1 for positive inputs. If you are already close to the threshold, a small change might have a large effect.\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz8.png\">\n",
    "\n",
    "For another reason that we will see later, we will not use simple thresholding, i.e., a **step function**, but rather a smoother function like the **sigmoid** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://what-if.xkcd.com/imgs/whatif-logo.png\">\n",
    "\n",
    "### What if all the non-linearities in an NN suddenly vanished?\n",
    "\n",
    "<small>(CREDITS: The following slide has been taken from AJ and ZA's tutorial):</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For now, let's simply ignore the bias term to simplify our notation. \n",
    "\n",
    "A neural network with an input layer, a middle layer, and an output layer computes the following:\n",
    "\n",
    "$$\\mathbf{y} = g(W^{(2)}g(W^{(1)}g(W^{(0)}\\mathbf{x})))$$\n",
    "\n",
    "$g$ is a non-linearity, which could be different for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we change $g$ to a linear function (e.g. a scaling factor), it can simply be multiplied into the weights matrices. Below we assume that $g = 1$, which allows us to simplify the expression:\n",
    "\n",
    "$$\\mathbf{y} = (W^{(2)}(W^{(1)}(W^{(0)}\\mathbf{x})))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since matrix multiplication is associative:\n",
    "\n",
    "$$A(BC) = (AB)C,$$\n",
    "\n",
    "we can get rid of the brackets altogether:\n",
    "\n",
    "$$\\mathbf{y} = W^{(2)}W^{(1)}W^{(0)}\\mathbf{x}.$$\n",
    "\n",
    "The series of linear transformations can be summarized in a single transformation matrix :\n",
    "\n",
    "$$T = W^{(2)}W^{(1)}W^{(0)}.$$\n",
    "\n",
    "And so the prediction of the neural network becomes:\n",
    "\n",
    "$$\\mathbf{y} = T\\mathbf{x}.$$\n",
    "\n",
    "The effective number of parameters in the now non non-linear neural network is $|\\mathbf{y}| \\times |\\mathbf{x}|$, which is precisely the same as a standard linear model.\n",
    "\n",
    "i.e., **the non-linearities are crucial**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Commonly-used activation functions\n",
    "Tanh: <img src=\"http://cs231n.github.io/assets/nn1/tanh.jpeg\">\n",
    "Sigmoid: <img src=\"http://cs231n.github.io/assets/nn1/sigmoid.jpeg\">\n",
    "ReLu: <img src=\"http://cs231n.github.io/assets/nn1/relu.jpeg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So, where do the weights come from?\n",
    "\n",
    "It's an **optimization** problem. We want to find the weights that \"work best\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/mountains_at_home.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training a Neural Network: Ingredients\n",
    "\n",
    "* we need to **define what \"works best\" means**\n",
    "* we need **a way to change the model (parameters, θ)** to get closer to a good model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining what works best ~ how close we are: Loss\n",
    "\n",
    "The loss measures how 'far' we are from true solution:\n",
    "\n",
    "$$L(\\mathbf{\\hat{y}},\\mathbf{y})$$\n",
    "\n",
    "For multi-class classification the **cross-entropy** is a commonly used loss function: \n",
    "\n",
    "$$L_{crossentropy}(\\mathbf{y},\\mathbf{\\hat{y}})= - \\sum_i y_i log(\\hat{y}_i)$$\n",
    "\n",
    "For regression the **mean squared error (MSE)** is a common loss function:\n",
    "$$L_{MSE}(\\mathbf{y},\\mathbf{\\hat{y}}) = \\frac{1}{n}\\sum_{i=1}^{n}({y-\\hat{y}})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to get closer to a good model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 1:** random guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 2:** start with some random initial parameters (weights), and randomly adjust them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 3:** follow the gradient: analytical method to find the best direction along which we should change our weight vector: **gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6d/Error_surface_of_a_linear_neuron_with_two_input_weights.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To sum up: Ingredients for training a Neural Network\n",
    "\n",
    "* we need to define what \"works best\" means \n",
    "    $\\rightarrow$ minimize some **loss**\n",
    "* we need a way to update the parameters to get closer to a good model\n",
    "    $\\rightarrow$ **minimize loss using a gradient-based method: gradient descent**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Intuitively, training a neural networks involves the following steps:\n",
    "\n",
    "* compute the gradient of the loss function with respect to the parameters\n",
    "* move the parameters in the negative direction of the gradient to decrease the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Skeleton of gradient descent:\n",
    "    \n",
    "**Input**: training set, loss function $L$\n",
    "\n",
    "Repeat for number of iterations (**epochs**): \n",
    " \n",
    "* do a forward pass\n",
    "* compute the loss on the data: $L(X,Y)$\n",
    "* compute the gradients: $\\mathbf{g} = L(X,Y)$ with respect to $w$ (backpropagation)\n",
    "* update the parameters, \"moving\" them in the negative direction of the gradient: $w = w - \\eta \\mathbf{g}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to get an intuitive understanding of the **backpropagation** algorithm. Backpropagation is a way of computing **gradients** of expressions by applying the **chain rule**. But before we get into details of gradients etc, let's introduce the **computational graph abstraction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can represent our neural network as a **computational graph**: nodes are operations, gray boxes are parameters.\n",
    "\n",
    "<img src=\"pics/yg-compgraph1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This corresponds to the neural network we have seen before (except for some dimensions, e.g. for input it is 150 instead of 4):\n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "<img src=\"pics/nn.png\" width=300> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Why a computational graph?\n",
    "\n",
    "It helps us to understand the flow of parameters in the model. \n",
    "\n",
    "What do we need to compute? The gradient of the loss function with respect to the parameters.\n",
    "\n",
    "**What's a gradient?** A vector of partial derivatives. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall: the **derivative** \n",
    "\n",
    "A derivative gives us a linear approximation of the function at a specific point. Intuitively, the derivative indicates the rate of change of a function $f$ with respect to a variable $x$ (surrounding the region around point $h$):\n",
    "\n",
    "    \n",
    "<img src=\"http://www.intuitive-calculus.com/images/what-is-a-derivative-4.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient\n",
    "\n",
    "We are interested in finding the gradient, i.e., all partial derivatives, since our functions are not just functions of single parameters, but of a lot of parameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example: gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's take a simple example of a function: $$f(x) = (x * y)$$ (or simply): $$f(x)=xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to calculate the gradient, the vector of partial derivatives (how much does the function change wrt the parameters x and y): $$\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The partial derivatives of this function are:\n",
    "\n",
    "$$f(x,y) = x y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = y \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## f(x) = x * y\n",
    "# let's take some numbers \n",
    "x = 4\n",
    "y = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12\n"
     ]
    }
   ],
   "source": [
    "## forward pass (function application)\n",
    "f = x * y\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "## the derivative wrt each variable tells us the sensitivity of the whole expression on its value. \n",
    "## for instance, take the partial derivative of f wrt y:\n",
    "df_dy = x  # it's simply x \n",
    "print(x) # this means if we increase the y by a tiny amount, the whole function would increase by this amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n"
     ]
    }
   ],
   "source": [
    "## similarly, the partial derivative of f wrt x is:\n",
    "df_dx = y\n",
    "print(y)  # changing x by some small amount would make the whole expression decrease (negative sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2: computational graph - compound expression\n",
    "\n",
    "(Thanks to lecture notes by Fei-Fei, Karphaty and Johnson, cf: http://cs231n.github.io/optimization-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's take the function: $$f(x,y,z) = (x + y) * z$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's represent it as a computational graph (green: forward pass values):\n",
    "<img src=\"pics/k1.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k2.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k3.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k4.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k5.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k6.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k7.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k8.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k9.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k10.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k11.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k13.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using the chain rule\n",
    "\n",
    "* we can compute the gradients of the loss along the backward path in our computational graph\n",
    "* once we know the gradients: we know how much we should change our parameters (in negative direction of gradients, as we want to minimize the loss): $w \\pm -\\eta \\mathbf{g}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "###### Training procedure\n",
    "\n",
    "Repeat for number of iterations (**batches**): \n",
    "* forward pass\n",
    "* compute loss on data: $L(y,\\hat{y})$\n",
    "* compute gradients: $\\mathbf{g} = L(y,\\hat{y})$ with respect to $w$\n",
    "* move parameters in opposite direction of gradient: $w \\pm -\\eta \\mathbf{g}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### In practice:\n",
    "\n",
    "* **gradient descent algorithm** (SGD, Adam, etc.)\n",
    "* **mini-batches** (use a small subset of training instances) (minibatch/batch size)\n",
    "* **further hyperparameters**: learning rate $\\eta$ (how big a step we take), number of epochs (how many times we go over the training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Input representations in neural networks / deep neural networks\n",
    "\n",
    "In deep learning we usually work with **dense** representations. Each feature is a vector of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The computational graph from above is actually not yet complete, it has no input and no loss! \n",
    "\n",
    "Hence, the first step is to extend it to define the input. In this case we **embed** the features (each feature gets a n-dimensional vector) and use that as input (here we use the concatenation of 3 word vectors):\n",
    "<img src=\"pics/yg-compgraph2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**a) sparse representation vs b) dense representation**\n",
    "\n",
    "<img src=\"pics/sparsevsdense.png\"> (Illustration from Goldberg's primer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The computational graph is still not complete, it misses the loss that is observed at the end of the forward pass, and used to backpropagate the error signal. Now we have a complete computational graph: <img src=\"pics/yg-compgraph3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* Yoav Goldberg's primer chapter 6: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\n",
    "* Fei-Fei, Karpathy and Johnson's lecture notes: http://cs231n.github.io/optimization-2/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
